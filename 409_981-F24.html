<html>

<head>
<title>CMPT 409/981 - Optimization for Machine Learning  (Fall 2024)</title>
<meta name="description" content="
Course webpage for CMPT 409/981 taught by Sharan Vaswani in Fall 2024."> 
</head>

<font face="helvetica">

<h1>CMPT 409/981 - Optimization for Machine Learning  (Fall 2024)</h1>

Lectures (beginning Sep 5): Tuesday (1.30 pm - 2.20 pm) and Thursday (12.30 pm - 2.20 pm) (SWH 10051).

<p>
Instructor: <a href="mailto:svaswani@sfu.ca">Sharan Vaswani</a>
<BR>
Instructor office hours: Thursday, 2.30 pm - 3.30 pm (TASC-1 8221)
</p>

<p> 
Teaching Assistant; <a href="mailto:qla96@sfu.ca">Qiushi Lin</a> <BR>
TA office hours: Monday, 9.30 am - 10.30 am (ASB 9814)  
</p>

<b>Course Objective</b>: Numerical optimization plays a key role in designing better algorithms for data science and artificial intelligence. This course introduces the foundational concepts of convex and non-convex optimization with applications to machine learning. It will give the students experience in 1. Proving theoretical guarantees for optimization algorithms, 2. Analyzing machine learning (ML) problems from an optimization perspective and 3. Developing and analyzing new optimization methods for ML applications. <p>

<b>Prerequisites</b>: Probability (e.g. CMPT 210, STAT 270, STAT 271), Linear Algebra (e.g. MATH 240), Basics of Multivariable Calculus (e.g. MATH 251). It is encouraged (though not necessary) for students to have taken (or be simultaneously taking) an introductory course on machine learning (e.g. CMPT 410/726). <p>

<b>Textbook</b>: There is no required textbook. We will use the following resources: 
<UL> 
<LI> Lectures on Convex Optimization, Nesterov, 2018
<LI> <a href = "https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"> Convex Optimization, Boyd and Vandenberghe, 2004 </a>
<LI> <a href = "https://arxiv.org/abs/1405.4980">Convex Optimization: Algorithms and Complexity, Bubeck, 2014</a>
<LI> Numerical Optimization, Nocedal and Wright, 2006 
<LI> First-order Methods in Optimization, Beck, 2017  
<LI> <a href = "https://arxiv.org/abs/1912.13213"> A Modern Introduction to Online Learning, Orabona, 2019 </a>
</UL>

<b>Grading</b>: Assignments 48%, Project 50%, Participation 2%
<p>
<a href="https://piazza.com/sfu.ca/fall2024/cmpt409981">Piazza</a> for course-related questions.
<p>

<h2>List of topics</h2>
<UL>
<LI> Basics: Subdifferentials, Optimality conditions, Lipschitz continuity, Convexity
<LI> (Non)-Convex minimization: (Projected) Gradient Descent, Nesterov/Polyak momentum, Mirror Descent, Newton method
<LI> (Non)-Convex minimization: Stochastic gradient descent (SGD), Variance reduction techniques, Adaptivity for SGD, Coordinate Descent
<LI> Applications to training ML models (logistic regression, neural networks)
<LI> Online optimization: Regret minimization, Follow the (regularized) leader, Adaptive gradient methods (AdaGrad, Adam, AMSGrad)
<LI> Applications to Imitation learning, Reinforcement learning
<LI> Min-Max optimization: Gradient Descent-Ascent, Extragradient
<LI> Applications to GANs, Robust optimization
</UL>

<h2>Schedule</h2>

<table width="80%" border="3" cellpadding="2">
<tbody><tr align="left">
<th>Date</th>
<th>Topics</th>
<th>Slides</th>
<th>References</th>
<th>Homework</th>
</tr>

<tr align="left">
<td> Thu September 5 </td>
<td> Course logistics, Lipschitz continuity, Smoothness	
<td> <a href="409_981-F24/L1.pdf">[L1]</a> </td>
<td> <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">[Matrix Cookbook]</a>
<a href="https://www.lkozma.net/inequalities_cheat_sheet/ineq.pdf">[List of inequalities]</a> 
<a href="https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/video_galleries/video-lectures/">[Linear Algebra Recap]</a>  </td>
<td> Assignment 0 released </td>
</tr>

<tr align="left">
<td> Tue September 10 </td>
<td> Gradient descent convergence for smooth non-convex functions 
<td> <a href="409_981-F24/L2.pdf">[L2]</a> </td>
<td> <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">[Multivariable Calculus]</a>  </td>
<td> </td>
</tr>

<tr align="left">
<td> Thursday Sep 12</td>
<td> Exact line-search, Convergence of GD with Back-tracking Armijo Line-search, Convex sets/functions
<td> <a href="409_981-F24/L3.pdf">[L3]</a> </td>
<td> Nocedal and Wright (3.1, 3.2), Boyd (2, 3)
<td> Assignment 0 due </td>
</tr>

<tr align="left">
<td> Tue September 17 </td>
<td> Convergence of GD for smooth, convex functions
<td> <a href="409_981-F24/L4.pdf">[L4]</a> </td>
<td> Bubeck (3.2, 3.4), <a href="https://www.cs.ubc.ca/~schmidtm/Courses/Notes/convex.pdf">[Convex Optimization Cheat Sheet]</a>  
<td> Assignment 1 released </td>
</tr>

<tr align="left">
<td> Thu September 19 </td>
<td> Nesterov acceleration and its convergence, Strongly-convex functions and GD convergence 
<td> <a href="409_981-F24/L5.pdf">[L5]</a> </td>
<td> Bubeck (3.7) </td>
<td> </td>	
</tr>

<tr align="left">
<td> Tue September 24</td>
<td> Heavy-Ball momentum 
<td> <a href="409_981-F24/L6.pdf">[L6]</a> </td>
<td> </td> 
<td> </td>
</tr>

<tr align="left">
<td> Thu September 26 </td>
<td> Convergence of Heavy-Ball momentum for quadratics, Preconditioned Gradient Descent and Newton method
<td> <a href="409_981-F24/L7.pdf">[L7]</a> </td>
<td> <a href="https://pages.cs.wisc.edu/~brecht/cs726docs/HeavyBallLinear.pdf">[Notes]</a> 
<td> Assignment 1 due </td>
</tr>

<tr align="left">
<td> Tue October 1</td>
<td> Convergence of Newton method for smooth, strongly-convex functions </td>
<td> <a href="409_981-F24/L8.pdf">[L8]</a> </td>
<td> Boyd (9.5) </td>
<td> </td>
</tr>

<tr align="left">
<td> Thu October 3</td>
<td> Dealing with constrained domains, Stochastic gradient descent and its convergence for smooth functions </td>
<td> <a href="409_981-F24/L9.pdf">[L9]</a> </td>
<td> <a href="https://xingyuzhou.org/blog/notes/Lipschitz-gradient">[Equivalent definitions of smoothness]</a>   
     <a href="https://xingyuzhou.org/blog/notes/strong-convexity">[Equivalent definitions of strong convexity]</a>  </td>
<td> Assignment 2 released </td>
</tr>

<tr align="left">
<td> Tue October 8 </td>
<td> Stochastic Gradient Descent and its convergence for smooth, convex functions
<td> <a href="409_981-F24/L10.pdf">[L10]</a> </td>
<td> </td>
<td> </td>
</tr>

<tr align="left">
<td> Thu October 10 </td>
<td> Convergence of SGD for smooth, strongly-convex functions, Interpolation
<td> <a href="409_981-F24/L11.pdf">[L11]</a> </td>
<td> <a href="https://arxiv.org/abs/1212.2002">[SGD proof for smooth, strongly convex functions]</a> 
<a href="https://arxiv.org/abs/1810.07288">[SGD under interpolation]</a>
<td> </td>
</tr>

<tr align="left">
<td> Tue October 15 </td>
<td colspan=3 style="color:blue"> Holiday </td>
<td> </td>
</tr>


<tr align="left">
<td> Thu October 17 </td>
<td> SGD under interpolation, Stochastic Line-Search
<td> <a href="409_981-F24/L12.pdf">[L12]</a> </td>
<td>  <a href="https://arxiv.org/abs/1905.09997">[Stochastic Line-Search]</a>   
<td> </td>
</tr>

<tr align="left">
<td> Tue October 22 </td>
<td colspan=3 style="color:blue"> No class </td>
<td> Project Proposal due </td>
</tr>

<tr align="left">
<td> Thu October 24 </td>
<td> Variance reduction & Convergence of SVRG, Lipschitz, convex functions </td>
<td> <a href="409_981-F24/L13.pdf">[L13]</a> </td>
<td> <a href="https://arxiv.org/abs/1901.08689">[SVRG]</a>, Bubeck (3.1) 
<td> Assignment 2 due </td>
</tr>

<tr align="left">
<td> Tue October 29</td>
<td> Convergence of Subgradient Descent, Introduction to Online Optimization
<td> <a href="409_981-F24/L14.pdf">[L14]</a> </td>
<td> Bubeck (3.1) </td>
<td> </td>	
</tr>


<tr align="left">
<td> Thu October 31</td>
<td> Online Convex Optimization, Online Gradient Descent, Online Mirror Descent
<td> <a href="409_981-F24/L15.pdf">[L15]</a> </td>
<td> Orabona (2.1 - 2.2, 4.1, 7.1-7.3) </td>
<td> </td>	
</tr>


<tr align="left">
<td> Tue November 5 </td>
<td> Online Mirror Descent </td>
<td> <a href="409_981-F24/L16.pdf">[L16]</a> </td>
<td> Bubeck (4.2-4.3) </td>
<td> Assignment 3 released </td>
</tr>


<tr align="left">
<td> Thu November 7</td>
<td> Follow the (regularized) leader </td>
<td> <a href="409_981-F24/L17.pdf">[L17]</a> </td>
<td> Orabona (4.1, 4.2, 7.1-7.3) </a> </td>  
<td> </td>
</tr>

<tr align="left">
<td> Tue November 12</td>
<td> AdaGrad and its regret bound </td>
<td> <a href="409_981-F24/L18.pdf">[L18]</a> </td>
<td> <a href="https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">[Original AdaGrad paper] <a href="https://arxiv.org/abs/1809.02864">[AdaGrad for smooth functions]</a> </td>
<td> </td>
</tr>


<tr align="left">
<td> Thu November 14</td>
<td> Convergence of AdaGrad, Adam </td>
<td> <a href="409_981-F24/L19.pdf">[L19]</a> </td>
<td> <a href="https://arxiv.org/abs/1412.6980">[Original Adam paper]</a> </td>
<td> </td>
</tr>

<tr align="left">
<td> Tue November 19 </td>
<td> Non-convergence of Adam </td>
<td> <a href="409_981-F24/L20.pdf">[L20]</a> </td>
<td> <a href="https://arxiv.org/abs/1904.09237">[Non-convergence of Adam and AMSGrad]</a>  </td>
<td> Assignment 4 released </td>
</tr>


<tr align="left">
<td> Thu November 21</td>
<td> Min-Max Optimization, Gradient Descent Ascent and its convergence, Extra Gradient </td>
<td> <a href="409_981-F24/L21.pdf">[L21]</a> </td>
<td> </td>
<td> </td>
</tr>

<tr align="left">
<td> Tue November 26</td>
<td> </td>
<td> </td>
<td> </td>
<td> </td>
</tr>


<tr align="left">
<td> Thu November 28 </td>
<td colspan="100%" style="color:blue">  Project Presentations </td>
</tr>

<tr align="left">
<td> Tue December 3 </td>
<td colspan="100%" style="color:blue">  Project Presentations </td>
</tr>

<tr align="left">
<td> Tue Dec 10 </td>
<td colspan="100%" style="color:blue">  Assignments 3, 4 due </td>
</tr>

<tr align="left"> 
<td> Tue Dec 17 </td>
<td colspan="100%" style="color:blue"> Final Project Report due </td>
</tr> 
     
</table>

<br>
<b> Related Courses </b>
<UL>    
<LI> <a href="https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/">UBC'22</a>
<LI> <a href="https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S20/">UBC'20</a>	
<LI> <a href="https://optml.mit.edu/teach/6881/">MIT</a> 
<LI> <a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc">UT Austin</a> 	
<LI> <a href="http://www.seas.ucla.edu/~vandenbe/236C/">UCLA</a> 	
<LI> <a href="https://stanford.edu/~boyd/cvxbook/">Stanford</a> 		
</UL> 
<p>
<BR>
</font>
</html>
