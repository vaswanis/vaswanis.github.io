<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="Sharan_Vaswani_CV.pdf">CV</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="team.html">Team</a></div>
<div class="menu-item"><a href="join_team.html">Join Team?</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>

<h3> <b> 2026 </b> </h3>

<p> &#x2022; Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model <br>
Xingtu Liu, Lin F. Yang, <b>Sharan Vaswani</b>, ALT 2026. <a href = "
https://arxiv.org/abs/2507.02089">[pdf]</a> <br>
"Constrained Optimization for Machine Learning" workshop, NeurIPS, 2025.
</p>

<h3> <b> 2025 </b> </h3>

<p> &#x2022; Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning <br> 
Reza Asad, Reza Babanezhad, <b>Sharan Vaswani</b>, ArXiv 2025. <a href = "https://arxiv.org/pdf/2509.09838">[pdf]</a> <br>
"Aligning Reinforcement Learning Experimentalists and Theorists" workshop, NeurIPS, 2025. </p> 


<p> &#x2022; Optimistic Actor-Critic with Parametric Policies: Unifying Sample Efficiency and Practicality <br> 
Max Qiushi Lin, Reza Asad, Kevin Tan, Haque Ishfaq, Csaba Szepesvari, <b>Sharan Vaswani</b> <br>
"Aligning Reinforcement Learning Experimentalists and Theorists" workshop, NeurIPS, 2025. <a href = "https://openreview.net/forum?id=qArzSifuhM">[pdf]</a> <br> 
</p>	

<p> &#x2022; Towards Parameter-Free Temporal Difference Learning <br>
Yunxiang Li, Mark Schmidt, Reza Babanezhad, <b>Sharan Vaswani</b> <br>
"Aligning Reinforcement Learning Experimentalists and Theorists" workshop, NeurIPS, 2025. <a href = "https://openreview.net/forum?id=BKcraYdfVd">[pdf]</a> <br>  </p>	

<p> &#x2022; Implicit Bias of Polyak and Line-Search Step Sizes on Linear Classification with Separable Data <br>
Chen Fan, Reza Babanezhad, Christos Thrampoulidis, Mark Schmidt, <b>Sharan Vaswani</b> <br>
"Optimization for Machine Learning" workshop, NeurIPS, 2025. <a href = "https://opt-ml.org/papers/2025/paper26.pdf">[pdf]</a> <br> 
</p>


<p> &#x2022; Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation <br>
Max Qiushi Lin, Jincheng Mei, Matin Aghaei, Michael Lu, Bo Dai, Alekh Agarwal, Dale Schuurmans, Csaba Szepesvari, <b>Sharan Vaswani</b>, ArXiv 2025.  <a href = "https://arxiv.org/abs/2505.03155">[pdf]</a> </p>

<p> &#x2022; Glocal Smoothness: Line Search can really help! <br>
Curtis Fox, Aaron Mishkin, <b>Sharan Vaswani</b>, Mark Schmidt, ArXiv 2025. <a href = "https://arxiv.org/abs/2506.12648">[pdf]</a>
</p>


<p> &#x2022; Armijo Line-search Can Make (Stochastic) Gradient Descent Provably Faster <br>
<b>Sharan Vaswani</b>, Reza Babanezhad, ICML 2025.  <a href = "https://arxiv.org/abs/2503.00229">[pdf]</a> </p>

<p> &#x2022; Preserving Plasticity in Continual Learning with Adaptive Linearity Injection <br>
Seyed Roozbeh Razavi Rohani, Khashayar Khajavi, Wesley Chung, Mo Chen, <b>Sharan Vaswani</b>, CoLLAs 2025. <a href = "https://arxiv.org/abs/2505.09486">[pdf]</a> 
</p>

<p> &#x2022; (Accelerated) Noise-adaptive Stochastic Heavy-Ball Momentum <br> 
Anh Dang, Reza Babanezhad, <b>Sharan Vaswani</b>, TMLR 2025. <a href = "https://arxiv.org/abs/2401.06738">[pdf]</a> <a href = "https://github.com/anh-dang/accelerated_noise_adaptive_shb">[code]</a> <br>
"Optimization for Machine Learning" workshop, NeurIPS, 2023. </p>

<p> &#x2022; Fast Convergence of Softmax Policy Mirror Ascent </br> 
Reza Asad, Reza Babanezhad, Issam Laradji, Nicolas Le Roux, <b>Sharan Vaswani</b>, AISTATS 2025. <a href = "https://arxiv.org/abs/2411.12042">[pdf]</a> <br>
"Optimization for Machine Learning" workshop, NeurIPS, 2024. </p> 

<h3> <b> 2024 </b> </h3>

<p> &#x2022; Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates <br>
Jincheng Mei, Bo Dai, Alekh Agarwal, <b>Sharan Vaswani</b>, Anant Raj, Csaba Szepesvari, Dale
Schuurmans, NeurIPS, 2024. <a href = "https://arxiv.org/abs/2502.07141">[pdf]</a>
</p>


<p> &#x2022; Improving OOD Generalization of Pre-trained Encoders via Aligned Embedding-Space Ensembles <br>
Shuman Peng, Arash Khoeini, <b>Sharan Vaswani</b>, Martin Ester. <br>
"Unifying Representations in Neural Models" workshop, NeurIPS, 2024. <a href = "https://openreview.net/pdf?id=RYSVQitYZY">[pdf]</a>
</p>


<p> &#x2022; From Inverse Optimization to Feasibility to ERM <br> 
Saurabh Mishra, Anant Raj, <b> Sharan Vaswani </b>, ICML, 2024. <a href = "https://arxiv.org/abs/2402.17890">[pdf]</a> <a href = "https://github.com/Saurabh-29/Inverse_Optimization_To_Feasibility_To_ERM">[code]</a> <a href = "https://www.youtube.com/watch?v=XvrJ6JMYA_U">[video]</a>
<br>
"Optimization for Machine Learning" workshop, NeurIPS, 2023. 
</p> 

<p> &#x2022; Towards Principled, Practical Policy Gradient for Bandits and Tabular MDPs <br> 
Michael Lu, Matin Aghaei, Anant Raj, <b>Sharan Vaswani</b>, RLC, 2024. 
<a href = "https://arxiv.org/abs/2405.13136">[pdf]</a>  <a href = "https://github.com/sudo-michael/practical-pg">[code]</a>  <a href="SPG-RL_Theory-slides.pdf">[slides]</a><a href ="https://www.youtube.com/watch?v=7lWpCZl-E4E">[Talk at Alberta]</a> <br>
"Optimization for Machine Learning" workshop, NeurIPS, 2023 <b>(Oral Presentation)</b> 
</p> 




<!-- <details>
   <summary>Abstract</summary>
   <div style="background-color:WhiteSmoke;padding:1%;">rrr $$sdgsg$$</div>
</details>    -->  


<h3> <b> 2023 </b> </h3>
<p> &#x2022; Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees <br>
<b>Sharan Vaswani</b>, Amirreza Kazemi, Reza Babanezhad, Nicolas Le Roux, NeurIPS, 2023. <a href = "https://arxiv.org/abs/2305.15249">[pdf]</a> <a href="DA-RL-slides.pdf">[slides] <a href = "https://www.youtube.com/watch?v=0RFheu-MGMg">[Talk at Vector]</a> <br>
“Duality Principles for Modern ML” workshop, ICML, 2023.
</p>


<p> &#x2022; Surrogate Minimization: An Optimization Algorithm for Training Large Neural Networks with Model Parallelism <br>
Reza Asad, Reza Babanezhad, Issam Laradji, Nicolas Le Roux, <b> Sharan Vaswani </b> <br> 
"Optimization for Machine Learning" workshop, NeurIPS, 2023. <a href = "https://openreview.net/forum?id=AtzAqJQpan">[pdf]</a> </p> 

<p> &#x2022; MSL: An Adaptive Momentem-based Stochastic Line-search Framework <br>
Chen Fan, <b> Sharan Vaswani </b>, Christos Thrampoulidis, Mark Schmidt. <br>"Optimization for Machine Learning" workshop, NeurIPS, 2023. <a href = "https://openreview.net/forum?id=UfvQbl7Kpx">[pdf]</a> </p>

<p> &#x2022; Target-based Surrogates for Stochastic Optimization <br>
	Jonathan Wilder Lavington<sup>*</sup>, <b>Sharan Vaswani<sup>*</sup></b>, Reza Babanezhad, Mark Schmidt, Nicolas Le Roux, ICML, 2023. <a href = "https://arxiv.org/abs/2302.02607">[pdf]</a> <a href = "Func-OPT-slides.pdf">[slides]</a> <br>
	"Optimization for Machine Learning" workshop, NeurIPS 2022.
</p>	
  
<h3> <b> 2022 </b> </h3>

<p> &#x2022; Near-Optimal Sample Complexity Bounds for Constrained MDPs <br>
 <b>Sharan Vaswani<sup>*</sup></b>, Lin F. Yang<sup>*</sup>, Csaba Szepesvari, NeurIPS, 2022. <a href = "https://arxiv.org/abs/2206.06270">[pdf]</a> <a href = "https://www.youtube.com/watch?v=dxRkOz6w7Pg">[Csaba's talk at Simons]</a>  
 </p> 

<p> &#x2022; Improved Policy Optimization for Online Imitation Learning <br>
	Jonathan Wilder Lavington, <b>Sharan Vaswani</b>, Mark Schmidt, CoLLAs, 2022. <a href = "https://arxiv.org/abs/2208.00088">[pdf]</a> <a href = "https://github.com/WilderLavington/Improved-Policy-Optimization-for-Online-Imitation-Learning">[code]</a> 
</p>	
 
 <p> &#x2022; Towards Painless Policy Optimization for Constrained MDPs <br>
 Arushi Jain<sup>*</sup>, <b>Sharan Vaswani<sup>*</sup></b>, Reza Babanezhad, Csaba Szepesvari, Doina Precup, UAI, 2022. <a href = "https://arxiv.org/abs/2204.05176">[pdf]</a> <a href = "https://github.com/arushijain94/CoinBettingPolitex">[code]</a>  
 </p>  

<p> &#x2022; Towards Noise-adaptive, Problem-adaptive (Accelerated) Stochastic Gradient Descent <br>
<b>Sharan Vaswani</b>, Benjamin Dubois-Taine, Reza Babanezhad, ICML, 2022 <b>(Oral Presentation)</b>. <a href = "http://arxiv.org/abs/2110.11442">[pdf]</a> <a href = "https://github.com/R3za/expsls">[code]</a> <a href="Exponential-slides.pdf">[slides]</a>  <br> 
"Optimization for Machine Learning" workshop, NeurIPS, 2021.
</p>  
  
<p> &#x2022; A general class of surrogate functions for stable and efficient reinforcement learning <br>
<b> Sharan Vaswani </b>, Olivier Bachem, Simone Totaro, Robert Muller, Shivam Garg, Matthieu Geist, Marlos Machado, Pablo Samuel Castro, Nicolas Le Roux, AISTATS, 2022 <b> (Best Paper Honorable Mention)</b>. <a href = "http://arxiv.org/abs/2108.05828">[pdf]</a> <a href = "https://github.com/svmgrg/fma-pg">[code]</a> <a href="FMAPG-slides.pdf">[slides]</a> <a href ="https://youtu.be/qHtNFUUimoM">[Talk at Alberta]</a> <br>
"Reinforcement Learning Theory" workshop, ICML 2021.
</p>  

<p> &#x2022; SVRG meets AdaGrad: Painless Variance Reduction <br>
Benjamin Dubois-Taine<sup>*</sup>, <b>Sharan Vaswani<sup>*</sup></b>, Reza Babanezhad, Mark Schmidt, Simon Lacoste-Julien, Machine Learning Journal 2022. <a href = "https://arxiv.org/abs/2102.09645">[pdf]</a> <a href = "https://github.com/bpauld/AdaSVRG">[code]</a>
</p> 

<h3> <b> 2021 </b> </h3>
       
<p> &#x2022; Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence <br>
Nicolas Loizou, <b> Sharan Vaswani </b>, Issam Laradji, Simon Lacoste-Julien, AISTATS, 2021. <a href = "https://arxiv.org/abs/2002.10542">[pdf]</a> <a href = "https://github.com/IssamLaradji/sps/">[code]</a> <br>
"Optimization for Machine Learning" workshop, NeurIPS 2020 <b> (Spotlight) </b>. </p>
  
<h3> <b> 2020 </b> </h3>

<p> &#x2022; Adaptive Gradient Methods Converge Faster with Over-Parameterization (but you should do a line-search) <br>
<b>Sharan Vaswani</b>, Issam Laradji, Frederik Kunstner, Si Yi Meng, Mark Schmidt, Simon Lacoste-Julien, arXiv, 2020. <a href = "https://arxiv.org/abs/2006.06835">[pdf] </a> <a href = "https://github.com/IssamLaradji/ada_sls">[code]</a> <br>
"Optimization for Machine Learning" workshop, NeurIPS 2020 <b> (Spotlight) </b>. </p>

<p> &#x2022; To Each Optimizer a Norm, To Each Norm its Generalization <br>
<b>Sharan Vaswani</b>, Reza Babanezhad, Jose Gallego, Aaron Mishkin, Simon Lacoste-Julien, Nicolas Le Roux, arXiv, 2020. <a href = "https://arxiv.org/abs/2006.06821">[pdf] </a> <br>
"Optimization for Machine Learning" workshop, NeurIPS 2020 <b> (Spotlight) </b>. </p> 	

<p> &#x2022; Old Dog Learns New Tricks: Randomized UCB for Bandit Problems <br>
<b>Sharan Vaswani</b>, Abbas Mehrabian, Audrey Durand, Branislav Kveton, AISTATS 2020. <a href = "https://arxiv.org/abs/1910.04928">[pdf]</a> <a href = "https://github.com/vaswanis/randucb/">[code]</a> <a href="RandUCB-slides.pdf">[slides]</a> </p>

<p> &#x2022; Fast and Furious Convergence: Stochastic Second Order Methods under Interpolation <br>
Si Yi Meng<sup>*</sup>, <b>Sharan Vaswani<sup>*</sup></b>, Issam Laradji, Mark Schmidt, Simon Lacoste-Julien, AISTATS 2020. <a href = "https://arxiv.org/abs/1910.04920">[pdf]</a> <a href = "https://github.com/IssamLaradji/ssn/">[code]</a> <a href="SSN-slides.pdf">[slides]</a> <br>
"Beyond First Order Methods in Machine Learning" workshop, NeurIPS 2019 <b> (Spotlight) </b>. </p>

<p> &#x2022; Combining Bayesian Optimization and Lipschitz Optimization <br>
Mohamed Osama Ahmed, <b>Sharan Vaswani</b>, Mark Schmidt, Machine Learning Journal 2020. <a href = "https://arxiv.org/abs/1810.04336">[pdf]</a></p>

<h3> <b> 2019 </b> </h3>

<p> &#x2022; Accelerating boosting via accelerated greedy coordinate 	descent <br> 
Xiaomeng Ju<sup>*</sup>, Yifan Sun<sup>*</sup>, <b>Sharan Vaswani<sup>*</sup></b>, Mark Schmidt. <br>
"Optimization for Machine Learning" workshop, NeurIPS 2019. <a href = "Boosting-Optimization.pdf">[pdf]</a> <a href = "https://github.com/xmengju/Accelerated_Boosting">[code]</a> </p>

<p> &#x2022;  Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates <br>
<b>Sharan Vaswani</b>, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, Simon Lacoste-Julien, NeurIPS, 2019. 
<a href = "https://arxiv.org/abs/1905.09997">[pdf]</a>  <a href = "https://github.com/IssamLaradji/sls/">[code]</a>   <a href = "https://github.com/IssamLaradji/sls/blob/master/neurips2019/SLS_Poster.pdf">[poster]</a>  <a href = "https://github.com/IssamLaradji/sls/blob/master/neurips2019/SLS_slides.pdf">[slides]</a> <a href = "https://www.youtube.com/watch?v=3Jx0tuZ1ERs"> [video]</a> </p>

<p> &#x2022; Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits <br>
Branislav Kveton, Csaba Szepesvari, <b>Sharan Vaswani</b>, Zheng Wen, Mohammad Ghavamzadeh, Tor Lattimore, ICML, 2019. <a href = "https://arxiv.org/abs/1811.05154">[pdf]</a> </p>

<p> &#x2022; Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron <br>
<b>Sharan Vaswani</b>, Francis Bach, Mark Schmidt, AISTATS, 2019.  <a href = "https://arxiv.org/abs/1810.07288">[pdf]</a> <a href = "SR-poster.pdf">[poster]</a> </p>

<h3> <b> 2018 </b> </h3>

<p> &#x2022; Structured Bandits and Applications <br>
Sharan Vaswani, PhD thesis, University of British Columbia. <a href="phd-thesis.pdf">[pdf]</a> <a href = "defense-slides.pdf">[slides]</a> </p>

<p> &#x2022; New Insights into Bootstrapping for Bandits <br> 
<b>Sharan Vaswani</b>, Branislav Kveton, Zheng Wen, Anup Rao, Mark Schmidt, Yasin Abbasi-Yadkori, arXiv, 2018. <a href = "https://arxiv.org/abs/1805.09793">[pdf]</a> </p>

<h3> <b> 2017 </b> </h3>

<p> &#x2022; Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback <br> 
Zheng Wen, Branislav Kveton, Michal Valko, <b>Sharan Vaswani</b>, NIPS, 2017. <a href = "IMB-IC.pdf">[pdf]</a> <a href = "IMB-IC-poster.pdf">[poster]</a><a href = "https://research.adobe.com/whose-likes-matter-most-ask-this-learning-algorithm/">[article] </a></p>

<p> &#x2022; Model-Independent Online Learning for Influence Maximization <br>
<b>Sharan Vaswani</b>, Branislav Kveton, Zheng Wen, Mohammad Ghavamzadeh, Laks.V.S.Lakshmanan, Mark Schmidt, ICML, 2017. <a href = "IMB.pdf">[pdf]</a> <a href = "IMB-slides.pdf">[slides]</a> <a href = "IMB-poster.pdf">[poster]</a> <a href = "IMB-code.zip">[code]</a></p>

<p> &#x2022; Horde of Bandits using Gaussian Markov Random Fields <br>
<b>Sharan Vaswani</b>, Mark Schmidt, Laks.V.S.Lakshmanan, AISTATS, 2017.<b> (Oral Presentation)</b> <a href = "HOB-GMRF.pdf">[pdf]</a> <a href = "HOB-GMRF-poster.pdf">[poster] </a> <a href = "HOB-GMRF-slides.pdf">[slides]</a></p>

<h3> <b> 2016 </b> </h3>

<p> &#x2022; Adaptive Influence Maximization: Why commit when you can adapt ? <br>
<b>Sharan Vaswani</b>, Laks.V.S.Lakshmanan, arXiv, 2016. <a href = "AIM.pdf">[pdf]</a></p>

<h3> <b> 2015 </b> </h3>

<p> &#x2022; Influence Maximization with Bandits <br>
<b>Sharan Vaswani</b>, Laks.V.S.Lakshmanan, Mark Schmidt. <br>
"Networks in the Social and Information Sciences" workshop, NIPS, 2015. <a href="IM_with_Bandits.pdf">[pdf]</a></p>

<p> &#x2022; Influence Maximization in Bandit and Adaptive Settings <br>
Sharan Vaswani, MSc thesis, University of British Columbia. <a href="msc_thesis.pdf">[pdf]</a></p>

<h3> <b> 2014 </b> </h3>

<p> &#x2022; Modeling Non-Progressive Phenomena for Influence Propagation <br>
Vincent Yun Lou, Smriti Bhagat, Laks V.S. Lakshmanan, <b>Sharan Vaswani</b>, ACM Conference on Online Social Networks (COSN) 2014. <a href="COSN_2014.pdf">[pdf]</a> <a href="COSN_tech_report.pdf">[ Tech_report ]</a> </p>

<h3> <b> Previous Research </b> </h3>

<p> &#x2022; Fast 3D Salient Region Detection in Medical Images using GPUs <br>
Rahul Thota, <b>Sharan Vaswani</b>, Amit Kale, and Nagavijayalakshmi Vydyanathan. Machine Intelligence and Signal Processing. Springer India, 2016. <a href="saliency.pdf">[pdf]</a></p>

<p> &#x2022; Performance Evaluation of Medical Imaging Algorithms on Intel MIC Platform <br>
Jyotsna Khemka, Mrugesh Gajjar, <b>Sharan Vaswani</b>, Nagavijayalakshmi Vydyanathan, Rama Malladi, Vinutha V. 20th IEEE International Conference on High Performance Computing (HiPC) 2013. <a href="HIPC_2013.pdf">[pdf]</a></p>

<p> &#x2022; Fast 3D Structure Localization in Medical Volumes using CUDA-enabled GPUs <br>
<b>Sharan Vaswani</b>, Rahul Thota, Nagavijayalakshmi Vydyanathan, Amit Kale. 2nd IEEE International Conference on Parallel Distributed and Grid Computing 2012, India. (<b>Best Paper Award</b>). <a href="PDGC_2012.pdf">[pdf]</a></p>

<div id="footer">
<div id="footer-text">
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
