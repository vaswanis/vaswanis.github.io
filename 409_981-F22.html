<html>

<head>
<title>CMPT 409/981 - Optimization for Machine Learning  (Fall 2022)</title>
<meta name="description" content="
Course webpage for CMPT 409/981 taught by Sharan Vaswani in Fall 2022."> 
</head>

<font face="helvetica">

<h1>CMPT 409/981 - Optimization for Machine Learning  (Fall 2022)</h1>

Lectures (beginning Sep 8): Monday (2.30 pm - 3.20 pm) (WMC 2200) and Thursday (2.30 pm - 4.20 pm) (AQ 5037).<p>
Instructor: <a href="mailto:svaswani@sfu.ca">Sharan Vaswani</a>
<BR>
Instructor office hours: Monday 4 pm - 5 pm (TASC-1 8221)
<p>

<b>Course Objective</b>: This course introduces the foundational concepts of convex and non-convex optimization with applications to machine learning. It will give the students experience in 1. Proving theoretical guarantees for optimization algorithms, 2. Analyzing machine learning (ML) problems from an optimization perspective and 3. Developing and analyzing new optimization methods for ML applications.
<p>

<b>Prerequisites</b>: Linear Algebra, Multivariable Calculus, (Undergraduate) Machine Learning
<p>

<b>Textbook</b>: There is no required textbook. We will use the following resources: 
<UL> 
<LI> Lectures on Convex Optimization, Nesterov, 2018
<LI> <a href = "https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"> Convex Optimization, Boyd and Vandenberghe, 2004 </a>
<LI> <a href = "https://arxiv.org/abs/1405.4980">Convex Optimization: Algorithms and Complexity, Bubeck, 2014</a>
<LI> Numerical Optimization, Nocedal and Wright, 2006 
<LI> First-order Methods in Optimization, Beck, 2017  
<LI> <a href = "https://arxiv.org/abs/1912.13213"> A Modern Introduction to Online Learning, Orabona, 2019 </a>
</UL>

<b>Grading</b>: Assignments 50%, Project 50%
<p>
<a href="https://piazza.com/sfu.ca/fall2022/cmpt409981">Piazza</a> for course-related questions.
<p>

<h2>List of topics</h2>
<UL>
<LI> Basics: Subdifferentials, Optimality conditions, Lipschitz continuity, Convexity
<LI> (Non)-Convex minimization: (Projected) Gradient Descent, Nesterov/Polyak momentum, Mirror Descent, Newton method
<LI> (Non)-Convex minimization: Stochastic gradient descent (SGD), Variance reduction techniques, Adaptivity for SGD
<LI> Applications to training ML models (logistic regression, kernel machines, neural networks)
<LI> Online optimization: Regret minimization, Follow the (regularized) leader, Adaptive gradient methods (AdaGrad, Adam)
<LI> Applications to Imitation learning, Reinforcement learning
<LI> Min-Max optimization: (Stochastic) Gradient Descent-Ascent, (Stochastic) Extragradient
<LI> Applications to GANs, Robust optimization, Multi-agent RL
</UL>

<h2>Schedule</h2>

<table width="80%" border="3" cellpadding="2">
<tbody><tr align="left">
<th>Date</th>
<th>Topics</th>
<th>Slides</th>
<th>References</th>
<th>Homework</th>
</tr>

<tr align="left">
<td> Thursday Sep 8</td>
<td> Course logistics, Lipschitz continuity, Smoothness
<td> <a href="409_981-F22/L1.pdf">[L1]</a> </td>
<td> <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">[Matrix Cookbook]</a>
<a href="https://www.lkozma.net/inequalities_cheat_sheet/ineq.pdf">[List of inequalities]</a> 
<a href="https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/video_galleries/video-lectures/">[Linear Algebra Recap]</a> 
</td>
<td> </td>
</tr>

<tr align="left">
<td> Monday Sep 12</td>
<td> Gradient descent convergence for smooth non-convex functions, Exact line-search
<td> <a href="409_981-F22/L2.pdf">[L2]</a> </td>
<td> 
<td> </td>
</tr>

<tr align="left">
<td> Thursday Sep 15</td>
<td> Convergence of GD with Back-tracking Armijo Line-search, Convex sets/functions
<td> <a href="409_981-F22/L3.pdf">[L3]</a> </td>
<td> Nocedal and Wright (3.1, 3.2), Boyd (2, 3)
<td> </td> 
</tr>
    
<tr align="left">
<td> Monday Sep 19</td>
<td colspan="100%" style="color:blue"> Holiday </td>
</tr>

<tr align="left">
<td> Thursday Sep 22</td>
<td> Convergence of GD for smooth, convex and strongly-convex functions
<td> <a href="409_981-F22/L4.pdf">[L4]</a> </td>
<td> Bubeck (3.2, 3.4), <a href="https://www.cs.ubc.ca/~schmidtm/Courses/Notes/convex.pdf">[Convex Optimization Cheat Sheet]</a>  
<td> Assignment 1 released </td>
</tr>

<tr align="left">
<td> Monday Sep 26</td>
<td> Projections onto convex sets, Projected GD, Nesterov acceleration and its convergence for smooth, convex functions
<td> <a href="409_981-F22/L5.pdf">[L5]</a> </td>
<td> Bubeck (3.7)
<td> </td>
</tr>

<tr align="left">
<td> Thursday Sep 29</td>
<td colspan="100%" style="color:blue"> Holiday </td>
</tr>

<tr align="left">
<td> Monday Oct 3</td>
<td> Heavy-Ball momentum and its convergence for quadratics
<td> <a href="409_981-F22/L6.pdf">[L6]</a> </td>
<td>  <a href="https://pages.cs.wisc.edu/~brecht/cs726docs/HeavyBallLinear.pdf">[Notes]</a>   
<td>  </td>
</tr>

<tr align="left">
<td> Thursday Oct 6</td>
<td> Newton method and its convergence for smooth, strongly-convex functions
<td> <a href="409_981-F22/L7.pdf">[L7]</a> </td>
<td> Boyd (9.5)
<td> Assignment 1 due </td>
</tr>

<tr align="left">
<td> Monday October 10</td>
<td colspan="100%" style="color:blue"> Holiday </td>
</tr>

<tr align="left">
<td> Thursday Oct 13</td>
<td> Stochastic Gradient Descent and its convergence for smooth (convex) functions
<td> <a href="409_981-F22/L8.pdf">[L8]</a> </td>
<td> 
<td>  </td>
</tr>

<tr align="left">
<td> Monday Oct 17</td>
<td> Stochastic Gradient Descent and its convergence for smooth, convex functions
<td> <a href="409_981-F22/L9.pdf">[L9]</a> </td>
<td> <a href="https://xingyuzhou.org/blog/notes/Lipschitz-gradient">[Equivalent definitions of smoothness]</a>   
     <a href="https://xingyuzhou.org/blog/notes/strong-convexity">[Equivalent definitions of strong convexity]</a>   
<td> Assignment 2 released </td>
</tr>

<tr align="left">
<td> Thursday Oct 20</td>
<td> Stochastic Gradient Descent and its convergence for smooth, strongly-convex functions, Interpolation
<td> <a href="409_981-F22/L10.pdf">[L10]</a> </td>
<td> <a href="https://arxiv.org/abs/1212.2002">[SGD proof for smooth, strongly convex functions]</a>   
     <a href="409_981-F22/L10-Extra.pdf">[More refined analysis]</a> </td>
<td> </td>
</tr>

<tr align="left">
<td> Monday Oct 24</td>
<td> Convergence of SGD for smooth, strongly-convex functions under interpolation, Convergence of SGD for smooth functions under strong growth condition
<td> <a href="409_981-F22/L11.pdf">[L11]</a> </td>
<td>  <a href="https://arxiv.org/abs/1810.07288">[SGD under interpolation]</a>   
<td>   </td>
</tr>


<tr align="left">
<td> Thursday Oct 27</td>
<td> Stochastic Line-Search and its convergence under interpolation, Variance reduction, SVRG and its convergence
<td> <a href="409_981-F22/L12.pdf">[L12]</a> </td>
<td>  <a href="https://arxiv.org/abs/1905.09997">[Stochastic Line-Search]</a>   
     <a href="https://arxiv.org/abs/1901.08689">[SVRG]</a>   
<td>  </td>
</tr>

<tr align="left">
<td> Monday Oct 31</td>
<td> Subgradient Descent and its convergence for Lipschitz, convex functions
<td> <a href="409_981-F22/L13.pdf">[L13]</a> </td>
<td> Bubeck (3.1)
<td> Project Proposal due </td>
</tr>


<tr align="left">
<td> Thursday Nov 3</td>
<td> Online Convex Optimization - Online Gradient Descent, Follow the (regularized) leader and their regret bounds
<td> <a href="409_981-F22/L14.pdf">[L14]</a> </td>
<td>  Orabona (2.1 - 2.2, 4.1, 7.1-7.3)
<td>  Assignment 2 due </td>
</tr>

<tr align="left">
<td> Monday Nov 7</td>
<td> AdaGrad and its regret bounds for convex, Lipschitz functions
<td> <a href="409_981-F22/L15.pdf">[L15]</a> </td>
<td> Orabona (4.2) <a href="https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">[Original AdaGrad paper]</a> </td>  
<td>  </td>
</tr>

<tr align="left">
<td> Thursday Nov 10</td>
<td> AdaGrad and its regret bound for convex, smooth functions;  Stochastic minimization of smooth, non-convex functions using AdaGrad Norm  
<td> <a href="409_981-F22/L16.pdf">[L16]</a> </td>
<td>  <a href="https://arxiv.org/abs/1809.02864">[AdaGrad for smooth functions]</a>  <a href="https://arxiv.org/abs/1809.02864">[AdaGrad Norm for smooth, non-convex functions] </a>  </td>
<td> Assignment 3 released </td>
</tr>


<tr align="left">
<td> Monday Nov 14</td>
<td> 
<td> 
<td> 
<td> </td>
</tr>

<tr align="left">
<td> Thursday Nov 17</td>
<td> 
<td> 
<td> 
<td> </td>
</tr>

<tr align="left">
<td> Monday Nov 21</td>
<td> 
<td> 
<td> 
<td> </td>
</tr>

<tr align="left">
<td> Thursday Nov 24</td>
<td> 
<td> 
<td> 
<td> </td>
</tr>

<tr align="left">
<td> Monday Nov 28</td>
<td colspan="100%" style="color:blue"> NeurIPS </td>
</tr>     
     
<tr align="left">
<td> Thursday Dec 1</td>
<td colspan="100%" style="color:blue"> NeurIPS </td>
</tr>

<tr align="left">
<td> Monday Dec 5 </td>
<td colspan="100%" style="color:red"> Project Presentations [2.30 pm - 6.30 pm in TASC-1 9204] </td>
</tr>

<tr align="left">
<td> Tuesday Dec 6 </td>
<td colspan="100%" style="color:red"> Project Presentations [1 pm - 5 pm in TASC-1 9204] </td>
</tr>
     
</table>

<br>
<b> Related Courses </b>
<UL>    
<LI> <a href="https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/">UBC'22</a>
<LI> <a href="https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S20/">UBC'20</a>	
<LI> <a href="https://optml.mit.edu/teach/6881/">MIT</a> 
<LI> <a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc">UT Austin</a> 	
<LI> <a href="http://www.seas.ucla.edu/~vandenbe/236C/">UCLA</a> 	
<LI> <a href="https://stanford.edu/~boyd/cvxbook/">Stanford</a> 		
</UL> 
<p>
<BR>
</font>
</html>
